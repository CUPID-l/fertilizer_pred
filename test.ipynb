{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "583fd6b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "583fd6b3",
        "outputId": "cadde8c0-7d14-428c-a643-6728f3529a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Fertilizer Class Index: 2\n",
            "Predicted Fertilizer: MOP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Redefine the same model\n",
        "# -------------------------\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def __init__(self, num_features, embed_dim):\n",
        "        super(FeatureTokenizer, self).__init__()\n",
        "        self.embed = nn.Linear(num_features, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, embed_dim=64, num_heads=4, num_layers=2):\n",
        "        super(FTTransformer, self).__init__()\n",
        "        self.tokenizer = FeatureTokenizer(num_features, embed_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)  # Tokenize input features\n",
        "        x = x.unsqueeze(1)     # Add sequence dimension\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(1)       # Remove sequence dimension\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# Load the model\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_features = 20\n",
        "num_classes = 7\n",
        "\n",
        "model = FTTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load('./fttransf_new.pth', map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# -------------------------\n",
        "# Prepare new input sample\n",
        "# -------------------------\n",
        "# Example: 23 values as input (topsoil + subsoil + deepsoil + Soil as encoded feature)\n",
        "new_input = np.array([\n",
        "    25.3, 55.2, 6.5, 90, 40, 60,     # Topsoil\n",
        "    14.1, 60.0, 6.4, 85, 38, 58,     # Subsoil\n",
        "    23.5, 12.0, 5.3, 0, 0, 23,     # Deepsoil\n",
        "    2, 5                                  # Encoded Soil and Crop\n",
        "], dtype=np.float32)\n",
        "\n",
        "\n",
        "input_tensor = torch.tensor(new_input).unsqueeze(0).to(device)  # shape [1, 23]\n",
        "\n",
        "# -------------------------\n",
        "# Predict\n",
        "# -------------------------\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    predicted_class = torch.argmax(output, dim=1).item()\n",
        "    print(\"Predicted Fertilizer Class Index:\", predicted_class)\n",
        "\n",
        "    # If you used LabelEncoder or custom mapping:\n",
        "    fertilizer_labels = {\n",
        "    0: \"DAP and MOP\",          # Source of Phosphorus and Potassium\n",
        "    1: \"Good NPK\",             # No fertilizer needed\n",
        "    2: \"MOP\",                  # Source of Potassium\n",
        "    3: \"Urea and DAP\",         # Source of Nitrogen and Phosphorus\n",
        "    4: \"Urea and MOP\",         # Source of Nitrogen and Potassium\n",
        "    5: \"Urea\",                 # Source of Nitrogen only\n",
        "    6: \"DAP\"                   # Source of Phosphorus only\n",
        "}\n",
        "    print(\"Predicted Fertilizer:\", fertilizer_labels[predicted_class])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
